{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:02:08.553559Z",
     "start_time": "2024-11-15T17:02:08.547251Z"
    }
   },
   "cell_type": "code",
   "source": "MODEL_VERSION = \"001\"",
   "id": "ecbf1c0687349949",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T17:02:15.994279Z",
     "start_time": "2024-11-15T17:02:08.559558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from code.classifier import PneumoniaClassifier\n",
    "from code.dataloader import PneumoniaDataset\n",
    "from code.custom_checkpoint import CustomModelCheckpoint\n",
    "from code.project_globals import TEST_DIR, TRAIN_DIR, VAL_DIR\n",
    "import pathlib\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:02:18.041734Z",
     "start_time": "2024-11-15T17:02:17.311205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#transform = transforms.Compose([transforms.Resize((60,60)),transforms.ToTensor()])\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "#print(TRAIN_DIR.as_posix())\n",
    "# Create dataset instances with proper arguments\n",
    "train = PneumoniaDataset(root_dir=TRAIN_DIR.as_posix(), transform=transform)\n",
    "test = PneumoniaDataset(root_dir=TEST_DIR.as_posix(), transform=transform)\n",
    "val = PneumoniaDataset(root_dir=VAL_DIR.as_posix(), transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=32, shuffle=False)\n",
    "\n",
    "# TensorBoard logger setup\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"pneumonia_classifier\")\n",
    "\n",
    "checkpoint_callback = CustomModelCheckpoint(\n",
    "    monitor='val_loss',         # Metric to monitor\n",
    "    dirpath='../checkpoints',    # Directory to save the checkpoints\n",
    "    filename='best_model-{epoch:02d}-{val_loss:.2f}',  # Save format\n",
    "    save_top_k=3,               # Only keep the 3 best models\n",
    "    mode='min'                  # Minimize val_loss\n",
    ")\n",
    "\n",
    "# Model setup\n",
    "model = PneumoniaClassifier(\n",
    "    backbone=models.resnet50(weights='ResNet50_Weights.DEFAULT'),\n",
    "    transfer_learning=True\n",
    ")\n",
    "\n",
    "# Trainer with TensorBoard logger\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=logger,  # Attach TensorBoard logger\n",
    "    log_every_n_steps=1,  # Log metrics after every step\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ],
   "id": "ba465dcd3a376771",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:02:18.516939Z",
     "start_time": "2024-11-15T17:02:18.316345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ],
   "id": "50279ad4027edea1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 15 18:02:18 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.90                 Driver Version: 565.90         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 Ti   WDDM  |   00000000:02:00.0  On |                  N/A |\n",
      "|  0%   41C    P8             11W /  285W |    3662MiB /  12282MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1744    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A      2000    C+G   ...Mozilla Thunderbird\\thunderbird.exe      N/A      |\n",
      "|    0   N/A  N/A      4980    C+G   ...on\\130.0.2849.80\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A      6272    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9068    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10212    C+G   ...on\\130.0.2849.80\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     10536    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11180    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     11864    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     12256    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13508    C+G   ...804_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A     14096    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     14644    C+G   C:\\Program Files\\NordVPN\\NordVPN.exe        N/A      |\n",
      "|    0   N/A  N/A     15936    C+G   ..._x64__kzf8qxf38zg5c\\Skype\\Skype.exe      N/A      |\n",
      "|    0   N/A  N/A     16976    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     19048    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     20756    C+G   ... Design\\DaVinci Resolve\\Resolve.exe      N/A      |\n",
      "|    0   N/A  N/A     23248    C+G   ...ogram Files\\Notepad++\\notepad++.exe      N/A      |\n",
      "|    0   N/A  N/A     23256    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     23492    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     30464    C+G   ...harm 243.21155.22\\bin\\pycharm64.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:28:17.101739Z",
     "start_time": "2024-11-15T17:02:18.750648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "id": "5401690dbbff9456",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type           | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | accuracy          | BinaryAccuracy | 0      | train\n",
      "1 | feature_extractor | Sequential     | 23.5 M | train\n",
      "2 | classifier        | Linear         | 4.1 K  | train\n",
      "-------------------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.049    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aszab\\miniconda3\\envs\\pneumonia_detection\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aszab\\miniconda3\\envs\\pneumonia_detection\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 163/163 [02:30<00:00,  1.08it/s, v_num=20]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 20.41it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 163/163 [02:31<00:00,  1.08it/s, v_num=20, val_acc_epoch=0.914]Learning Rate after epoch 0: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aszab\\miniconda3\\envs\\pneumonia_detection\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 163/163 [02:35<00:00,  1.05it/s, v_num=20, val_acc_epoch=0.914, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 25.64it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.957, train_acc_epoch=0.000]Learning Rate after epoch 1: 0.001\n",
      "Epoch 2: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.957, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 17.24it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.963, train_acc_epoch=0.000]Learning Rate after epoch 2: 0.001\n",
      "Epoch 3: 100%|██████████| 163/163 [02:35<00:00,  1.05it/s, v_num=20, val_acc_epoch=0.963, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.968, train_acc_epoch=0.000]Learning Rate after epoch 3: 0.001\n",
      "Epoch 4: 100%|██████████| 163/163 [02:34<00:00,  1.06it/s, v_num=20, val_acc_epoch=0.968, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 14.29it/s]\u001B[A\n",
      "Epoch 4: 100%|██████████| 163/163 [02:34<00:00,  1.05it/s, v_num=20, val_acc_epoch=0.972, train_acc_epoch=0.000]Learning Rate after epoch 4: 0.0005\n",
      "Epoch 5: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.972, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 20.83it/s]\u001B[A\n",
      "Epoch 5: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.974, train_acc_epoch=0.000]Learning Rate after epoch 5: 0.0005\n",
      "Epoch 6: 100%|██████████| 163/163 [02:31<00:00,  1.07it/s, v_num=20, val_acc_epoch=0.974, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 16.13it/s]\u001B[A\n",
      "Epoch 6: 100%|██████████| 163/163 [02:32<00:00,  1.07it/s, v_num=20, val_acc_epoch=0.976, train_acc_epoch=0.000]Learning Rate after epoch 6: 0.0005\n",
      "Epoch 7: 100%|██████████| 163/163 [02:35<00:00,  1.05it/s, v_num=20, val_acc_epoch=0.976, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.18it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.979, train_acc_epoch=0.000]Learning Rate after epoch 7: 0.0005\n",
      "Epoch 8: 100%|██████████| 163/163 [02:32<00:00,  1.07it/s, v_num=20, val_acc_epoch=0.979, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.87it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 163/163 [02:32<00:00,  1.07it/s, v_num=20, val_acc_epoch=0.978, train_acc_epoch=0.000]Learning Rate after epoch 8: 0.0005\n",
      "Epoch 9: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.978, train_acc_epoch=0.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 21.74it/s]\u001B[A\n",
      "Epoch 9: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.977, train_acc_epoch=0.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate after epoch 9: 0.00025\n",
      "Epoch 9: 100%|██████████| 163/163 [02:36<00:00,  1.04it/s, v_num=20, val_acc_epoch=0.977, train_acc_epoch=0.000]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T22:49:13.016083Z",
     "start_time": "2024-11-15T22:49:12.951532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint_path = '../checkpoints/best_model-epoch=07-val_loss=0.12.ckpt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Access the metadata\n",
    "metadata = checkpoint.get(\"metadata\", {})\n",
    "print(f\"Checkpoint metadata: {metadata}\")\n",
    "\n",
    "# Restore the model state\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ],
   "id": "d6b8db71d352177f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aszab\\AppData\\Local\\Temp\\ipykernel_2044\\973869640.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../checkpoints/best_model-epoch_10-val_loss_0.25.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m checkpoint_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../checkpoints/best_model-epoch_10-val_loss_0.25.ckpt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 2\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(checkpoint_path)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Access the metadata\u001B[39;00m\n\u001B[0;32m      5\u001B[0m metadata \u001B[38;5;241m=\u001B[39m checkpoint\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pneumonia_detection\\Lib\\site-packages\\torch\\serialization.py:1319\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m   1317\u001B[0m     pickle_load_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1319\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_file_like(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m   1320\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m   1321\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1322\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1323\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1324\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pneumonia_detection\\Lib\\site-packages\\torch\\serialization.py:659\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    657\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    658\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 659\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _open_file(name_or_buffer, mode)\n\u001B[0;32m    660\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    661\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pneumonia_detection\\Lib\\site-packages\\torch\\serialization.py:640\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 640\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mopen\u001B[39m(name, mode))\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../checkpoints/best_model-epoch_10-val_loss_0.25.ckpt'"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
